---
title: "code"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

```{r Packages}
library(tidyverse)
library(purrr)
library(gapr)
library(glue)
```


# Setup
This work should all be housed in a GitHub repo. Establish the repo and add your collaborators. 

Loading the data takes a minute, so I would suggest you do it once and cache it. This just means including `knitr::opts_chunk$set(cache = TRUE)` in one of your chunk options.

The problem with caching is that sometimes results of a later chunk depend upon earlier ones and things can get out of sync. If you make a change and it doesn't have the result you expect, try clearing the cache and knitting again.
a
If this is confusing, don't worry about it. Either check in with me or ignore this part and don't use caching. It will just take a little longer to render your file each time you click "knit".

I would also recommend not tracking the cache files. This means adding the cache folder to your `.gitignore` file. If you initialized your repo with the R `.gitignore` file this is actually already taken care of for you. If not, add `/*_cache/` to your `.gitignore`.

# Part A: Data 
### 20 points

The following function downloads data from the Oregon Department of education
website on the number of students who scored in each performance category on the
statewide assessment by race/ethnicity for every school in the state. It takes
one argument, `year`, which must be a two digit integer from 15 to 18 (representing the 2014-15 to 2017-18 school years). It actually won't work on later years because the file structure changed and I wanted to keep the function simple.

**NOTE:** This function uses the `glue` function from the package of the same name. If you do not already have this package installed, please first install it with `install.packages("glue")`. It also uses `{rio}` for the import, which you should already have installed, but if not, install that first too.


```{r}
download_file <- function(year) {
  link <- glue::glue("https://www.oregon.gov/ode/educator-resources/assessment/TestResults20{year}/pagr_schools_ela_raceethnicity_{year-1}{year}.xlsx")
  rio::import(link, setclass = "tibble", na = c("-", "--", "*"))
}
```

1. (10 points) Use the function above to download *all* of the data from the 2014-15 to 2017-18 school years and bind it into a single data frame, **using a single function** (i.e., one line of code). Note, this may take a minute or two to run, depending on your Internet speed.

```{r Question A1.1}
data <- map_df(seq(15,18,1), download_file)

head(data)
```

Conduct some basic data cleaning to make your data file look like the following. 

* Filter for only student groups coded as `"White"` or `"Hispanic/Latino"`. 
* Select variables related to the number of students in each of the levels (1:4), and not percentages or collapsed levels.
* Remove any row that has missing data in any of the *n* variables

```{r Question A1.2, cache = T}
data <- data %>% 
  # Clean column names
  janitor::clean_names() %>% 
  # Filter for White and Hispanic/Latino students
  mutate(
    student_group = factor(student_group),
    academic_year = factor(academic_year),
    district = factor(district),
    grade_level = factor(grade_level)
  ) %>% 
  filter(
    student_group %in% c("White", "Hispanic/Latino")
  ) %>% 
  # Select levels columns
  select(academic_year, district, school, student_group, grade_level, 
         number_level_1, number_level_2, number_level_3, number_level_4) %>% 
  # Creating data frame to match example, removing NAs
  pivot_longer(cols = c(number_level_1, number_level_2, number_level_3, number_level_4), 
               names_to = "level",
               values_to = "n",
               values_drop_na = T) %>% 
  mutate(
    level = gsub("number_level_", "", level) %>% 
      as.factor()
  )
```

2. (10 points) Sum the *n* across grades and school for each district within each academic year, student group, and level. Calculate the number of schools (unique schools listed) for each district. Also move your data to a wider format where the student groups become new columns, filled by the district n. Remove districts that did not report numbers for one or both student groups (i.e., drop missing data from the new columns). Your data frame should look like the below


```{r Question A2}
data <- data %>% 
  select(academic_year, district, school, student_group, level, n)

d <- split(data, data$district)

# Unique Schools

### FUNCTION
n_school_funct <- function(df){
  df %>% 
    mutate(
      n_school = unique(df$school) %>% 
        length()
    )
}

### RUN
d <- map(d, n_school_funct)

# Sum n across grades and school for each district within each academic year, student group, and level
sum_function <- function(df){
  df %>% 
    select(academic_year, district, n_school, student_group, level, n) %>% 
    group_by(academic_year, student_group, level) %>% 
    mutate(
      n = sum(n)
    ) %>% 
    unique() %>% 
    pivot_wider(names_from = "student_group",
                          values_from = "n") %>% 
            janitor::clean_names() 
}

d <- map(d, sum_function)
  

d <- Filter(function(x) ncol(x) == 6, d)

# Reduce to single dataframe

### FUNCTIONS
distinct_function <- function(df){
  df %>% 
    select(academic_year, district, level, n_school, hispanic_latino, white) %>% 
    dplyr::distinct()
}

### RUN
d <- do.call(rbind, d)

d <- d %>% 
  drop_na(hispanic_latino, white)

d <- distinct_function(d)

```

# Part B: Achievement disparities
### 40 points

If you have not already done so, please install the [{gapr}](https://github.com/datalorax/gapr) package using the following code


```{r}
remotes::install_github("datalorax/gapr")
```

The {gapr} package includes, at the moment, one function, `estimate_v()`, which estimates the average difference between two distributions in terms of an effect size when the only data available are counts within bins (for more information, see [Ho & Reardon, 2012](https://journals.sagepub.com/doi/10.3102/1076998611411918)). The nice thing about this approach is that we're able to obtain the effect size of average differences in achievement between two groups of students *as if we had the full, student level data* even though we just have counts within bins (performance levels).

The `estimate_v` function takes 3 arguments in the following order: (1) the data frame including the counts for each group, (2) a character string stating the name of the column with the counts for the reference group, and (3) a character string stating the name of the column with the counts for the focal group.

To estimate the average achievement difference between students coded Hispanic/Latino versus White for the first school district in the first year in our data, we would run the following

```{r}
library(gapr)
d[1:4, ] %>% # data source - just first four rows for first district
  estimate_v("white", "hispanic_latino") # columns for reference/focal groups
```

```{r}
##         auc          v       v_se
## 1 0.2824054 -0.8141778 0.07283486
```

We can see that students coded Hispanic/Latino scored, on average, about 0.81 
standard deviations below students coded White within this school district for this year.

1. (10 points) Estimate the average achievement differences in terms of an effect size for every school district in the state that reported data on both student groups (i.e., using the data we created above), for each academic year. Conduct this analysis twice, once using `group_by() %>% nest() map()` and once using `nest_by() %>% summarize()`.

```{r Question B1.1 group_by}
models_group_by <- d %>%
  group_by(district, academic_year, n_school) %>% 
    nest()

mgp_output <- models_group_by %>% 
  mutate(
    output = map(data, ~estimate_v(.x, "white", "hispanic_latino"))
  ) %>% 
  select(district, academic_year, n_school, output) %>% 
  unnest(cols = c(output))

```

```{r Question B1.2 nest_by}
models_nest_by <- d %>%
  ungroup() %>% 
  nest_by(district, academic_year, n_school) 

mnb_output <- models_nest_by %>% 
  summarise(
    output = estimate_v(data, "white", "hispanic_latino")
  ) %>% 
  mutate(
    auc = output$auc,
    v = output$v,
    v_se = output$v_se
  ) %>% 
  select(district, academic_year, n_school, auc, v, v_se)
```

Note, on my computer these took about 40 seconds per analysis. It will likely take a bit longer for you, but shouldn't take *too* long. You may get a warning. Just go ahead and ignore that for now.

2. (5 points)

Reproduce the following plot to confirm that the estimates are equivalent across methods.

```{r Question B2}
b2 <- tibble(
  nest_by_data = mnb_output$v,
  group_by_data = mgp_output$v
)
ggplot(b2, aes(x = nest_by_data, y = group_by_data)) +
  geom_point() +
  geom_smooth()
```

3. (15 points) The plot below shows the achievement difference estimates for one school district across years. Produce a similar plot to the below for each of the first 100 school districts. Once you've created all the plots, **reproduce them again using an alternative method**. Make sure you don't miss the caption noting the number of schools in the district. 

Hint: You need to select unique *districts*. You may want to try something like the following

```{r}
models_group_by <- d %>%
  group_by(district) %>% 
    nest()
```


**Extra Credit:** For up to one point extra credit, create a third set that "staples together" each of the preceding pairs of plots (i.e., to confirm that the plots were produced identically by both methods).

4. (10 points) Save the plots into a "plots" directory. Make sure the file names are meaningful.

**HINTS**

* You don't have to use a loop to create the file names (maybe give `{glue}` a try? Otherwise `paste` or `paste0` will work fine).
* When working with the code, limit the number of plots you're saving to, say, the first five to make sure it works before running it on all plots.


